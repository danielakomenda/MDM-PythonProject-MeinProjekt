{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362a80bf-818b-45be-a73e-1b1beb3b4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import pickle\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels\n",
    "import statsmodels.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f637ac70-af31-4a65-a6ad-e37796c9794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/missd/Desktop/6. Semester/6. Model Deployment Maintenance/MDM - Python Projekt/src/\")\n",
    "import mdm_python.data_preparation.db_entsoe as db_entsoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ac8f2b-a232-48b6-bdea-7820c68bd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = Path(\"../data/models\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9040ff09-0bf2-4ce0-8808-785f674efe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metaparams_local():\n",
    "    metaparams = dict()\n",
    "    energy_types = [\"solar\", \"nuclear\", \"wind\", \"water_river\", \"water_pump\", \"water_reservoir\"]\n",
    "    for type in energy_types:\n",
    "        with open(model_directory/f'metaparams_{type}.pickle', 'rb') as file:\n",
    "            metaparam = pickle.load(file)\n",
    "            metaparams[type]=metaparam\n",
    "    return metaparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ccab07-0add-4cfa-afaa-094756f77f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Transform the data to log-scale with an offset to handle the high number of zeros\n",
    "    The offset will change the skew-of the histogram close to 0\n",
    "    \"\"\"\n",
    "    data = data.drop(columns=\"total\")\n",
    "    \n",
    "    offset = dict(\n",
    "        wind = 1.4,\n",
    "        solar = 6,\n",
    "        water_reservoir = 900,\n",
    "        water_river = 150,\n",
    "        water_pump = 700\n",
    "    )\n",
    "    data_transformed = data.apply(lambda col: np.log10(col+offset[col.name]) if col.name in offset else col)\n",
    "    data_transformed = data_transformed.resample(\"W\").mean()\n",
    "    \n",
    "    \"\"\"\n",
    "    Create new DataFrame with the missing days in the index and NaN-Values\n",
    "    Fill NaN-Values with the average between the previous and the next value\n",
    "    \"\"\"\n",
    "    index_date = pd.date_range(start=data_transformed.index[0], end=data_transformed.index[-1], freq='W')\n",
    "    data_fixed = data_transformed.reindex(index_date)\n",
    "    data_fixed = data_fixed.interpolate(method='linear')\n",
    "    data_fixed = data_fixed.dropna()\n",
    "    assert data_fixed.index.freq is not None, \"Data must still be fixed-frequency\"\n",
    "\n",
    "    dict_of_transformed_data = dict()\n",
    "\n",
    "    for col_name in data_fixed.columns:\n",
    "        values = SimpleNamespace(\n",
    "            name = col_name,\n",
    "            transformed_values = data_fixed[col_name],\n",
    "            offset = offset.get(col_name),\n",
    "        )\n",
    "        dict_of_transformed_data[col_name] = values\n",
    "    \n",
    "    return dict_of_transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096578f3-0736-4f7c-93f6-c9b936b0d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_model(dataset):\n",
    "    \"\"\"\n",
    "    Load the stored Meta-Parameters\n",
    "    Calculate the model with the actual data\n",
    "    Return production model\n",
    "    \"\"\"\n",
    "    metaparams = load_metaparams_local()\n",
    "    \n",
    "    for name, values in dataset.items():\n",
    "        series = values.transformed_values\n",
    "        params = metaparams[name]\n",
    "        values.production_model = statsmodels.api.tsa.statespace.SARIMAX(\n",
    "            series,\n",
    "            trend=params[\"trend\"],\n",
    "            order=(params[\"p\"], params[\"d\"], params[\"q\"]),\n",
    "            seasonal_order=(1,1,0,52),\n",
    "        ).fit(\n",
    "            disp=False,\n",
    "            cov_type='none',\n",
    "            full_output=False,\n",
    "            low_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{name} is done\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16970c55-c112-4e16-a488-21d7820f1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_production_model(dataset):\n",
    "    for name, values in dataset.items():\n",
    "        model_directory.mkdir(parents=True, exist_ok=True)\n",
    "        with open(model_directory/f\"{name}.pickle\", \"wb\") as fh:\n",
    "            pickle.dump(values, fh)\n",
    "            print(f'Model for {name} is stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9a421f-d969-4f3f-a065-f8a6dccb4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_azure():\n",
    "\n",
    "    dotenv.load_dotenv()\n",
    "    azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(\n",
    "        azure_storage_connection_string\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Create new Container with latest Suffix\n",
    "    \"\"\"\n",
    "    exists = False\n",
    "    containers = blob_service_client.list_containers(include_metadata=True)\n",
    "    suffix = 0\n",
    "    for container in containers:\n",
    "        existingContainerName = container[\"name\"]\n",
    "        if existingContainerName.startswith(\"energy-model\"):\n",
    "            parts = existingContainerName.split(\"-\")\n",
    "            newSuffix = int(parts[-1])\n",
    "            if newSuffix > suffix:\n",
    "                suffix = newSuffix\n",
    "    suffix += 1\n",
    "    container_name = f\"energy-model-{suffix}\"\n",
    "    print(\"new container name: \")\n",
    "    print(container_name)\n",
    "    \n",
    "    for container in containers:\n",
    "        print(\"\\t\" + container[\"name\"])\n",
    "        if container_name in container[\"name\"]:\n",
    "            print(\"EXISTIERTT BEREITS!\")\n",
    "            exists = True\n",
    "    if not exists:\n",
    "        container_client = blob_service_client.create_container(container_name)\n",
    "\n",
    "    \n",
    "    models = dict(\n",
    "        nuclear_model=model_directory / \"nuclear.pickle\",\n",
    "        solar_model=model_directory / \"solar.pickle\",\n",
    "        water_pump_model=model_directory / \"water_pump.pickle\",\n",
    "        water_reservoir_model=model_directory / \"water_reservoir.pickle\",\n",
    "        water_river_model=model_directory / \"water_river.pickle\",\n",
    "        wind_model=model_directory / \"wind.pickle\",\n",
    "    )\n",
    "\n",
    "    for model, file_path in models.items():\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, blob=file_path\n",
    "        )\n",
    "        print(f\"\\nUploading to Azure Storage as blob:\\n\\t{file_path}\")\n",
    "    \n",
    "        # Upload the created file\n",
    "        with open(file=file_path, mode=\"rb\") as data:\n",
    "            blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ef0743-173e-42dc-81aa-b07c14453b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    energy_data = db_entsoe.extract_daily_energy()\n",
    "    print(\"raw_data loaded\")\n",
    "    dataset = prepare_raw_data(energy_data)\n",
    "    print(\"data transformed\")\n",
    "    dataset = create_production_model(dataset)\n",
    "    print(\"metaparams applied\")\n",
    "    store_production_model(dataset)\n",
    "    print(\"stored locally\")\n",
    "    store_to_azure()\n",
    "    print(\"stored in cloud\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
