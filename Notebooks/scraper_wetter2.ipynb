{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c59665da-e6c8-44b2-9f5a-db28f1eb4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "import pymongo\n",
    "import httpx\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34b86bb5-9c9d-41f8-935d-954fb88ec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get MongoDB-URI\n",
    "    mongodb_uri = os.getenv(\"MONGODB_URI\")\n",
    "    DBclient = pymongo.MongoClient(mongodb_uri)\n",
    "    db = DBclient[\"MDM-Python-MeinProjekt\"]\n",
    "\n",
    "    if \"Wetter\" in db.list_collection_names():\n",
    "        return db[\"Wetter\"]\n",
    "    else:\n",
    "        collection = db[\"Wetter\"]\n",
    "        collection.create_index([\n",
    "            (\"location\", pymongo.ASCENDING),\n",
    "            (\"datetime\", pymongo.ASCENDING),\n",
    "        ], unique=True)\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d65a990d-3cce-481c-bdc3-a34430f474aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36 Locations in Switzerland\n",
    "locations = {\n",
    "    \"Aarau,Switzerland\": \"5a5ecc58df562835e3fcbae5f8c52e64e7247918\",\n",
    "    \"Appenzell,Switzerland\": \"162a2967db482d9de5e1db7b98c7fa5a779f2875\",\n",
    "    \"Basel,Switzerland\": \"e502a0a8fc421e6a8f9f1c4034f6b46ff6f59f62\",\n",
    "    \"Bellinzona,Switzerland\": \"f398035eff9921de0368cc494578468b1d5c99ad\",\n",
    "    \"Bern,Switzerland\": \"94fa2a5396a4723b31142ab413d3ec1be77d62d8\",\n",
    "    \"Chur,Switzerland\": \"6c0da286ee836415b66b138d6f13076fbcdb3899\",\n",
    "    \"Davos,Switzerland\": \"e260b1b791f27abac6a4f7771a2401be6aee67a9\",\n",
    "    \"Delemont,Switzerland\": \"f661d8d34fbb2431d6f02a9613c09a3782655d55\",\n",
    "    \"Einsiedeln,Switzerland\": \"14ee1f6a8ff571616de7f378af94b0a588c67bda\",\n",
    "    \"Frauenfeld,Switzerland\": \"8bf573628b0fd96382f03bef84c063f3aa481e21\",\n",
    "    \"Fribourg,Switzerland\": \"f5240fc4fcffbe2c1680ee6f026349a7c2c0da48\",\n",
    "    \"Geneva,Switzerland\": \"eacb20159164fa15c55f87d7d66068b4b2ceaf39\",\n",
    "    \"Glarus,Switzerland\": \"dcb4a71fe438a4165ee26a4d370d259a2c5896d0\",\n",
    "    \"Grindelwald,Switzerland\": \"ab1c057337ed2fca115ed0e16b4ec2467466aaad\",\n",
    "    \"La_Chaux_De_Fonds,Switzerland\": \"d78dc7da556213f33175d58cc6d1de8534ac3a6c\",\n",
    "    \"Laax,Switzerland\": \"eb8dfc2322338b344ef7f8f1063e01f91b555137\",\n",
    "    \"Lausanne,Switzerland\": \"fb15d1e5d748ce024a944f59f9bf651919032bd1\",\n",
    "    \"Lauterbrunnen,Switzerland\": \"8a9e5db01b10728c85ac4944d19e79e515f3deba\",\n",
    "    \"Lenzerheide,Switzerland\": \"73330944ee4dbe329e5f8cfbf7bc49a0f420ba2f\",\n",
    "    \"Leukerbad,Switzerland\": \"7d2f2e013101f38faf031a38c5d9b348aeb883e3\",\n",
    "    \"Lucerne,Switzerland\": \"470964fac0430b6083c52ddd3e2a400c542e0e60\",\n",
    "    \"Lugano,Switzerland\": \"f9bc6e13323ac7547a608fa227ff7e274284d1f2\",\n",
    "    \"Montreux,Switzerland\": \"0034c59b5b977acd17fe5837e5845ae9f41e5a09\",\n",
    "    \"Neuchatel,Switzerland\": \"684590561854da6d27a36d9e659cc4739f675b1c\",\n",
    "    \"Romanshorn,Switzerland\": \"39fe927062dc9f07e6b3abed0189caf29b9745fd\",\n",
    "    \"Saas_Fee,Switzerland\": \"865c64f03112f377ad70f301a218110eefa322b8\",\n",
    "    \"Sarnen,Switzerland\": \"8a7c9fe3ae4cbce26ca0d3447542641f60ef781c\",\n",
    "    \"Savognin,Switzerland\": \"8bb604df8ccd428e2f37e5c4239c93f4dc55f19f\",\n",
    "    \"Schaffhausen,Switzerland\": \"8b5f38ff71ef8dcc2b857f39361149bb6193e4c3\",\n",
    "    \"Sion,Switzerland\": \"f092059ab917cc1a9a335215a1f4744944feaec3\",\n",
    "    \"Solothurn,Switzerland\": \"a6e3ca1b9fedf679fcc41159f5f5a56a58ca7354\",\n",
    "    \"St_Gallen,Switzerland\": \"c6a7895be45659cd932d951b975b522d5964f9af\",\n",
    "    \"Tenero,Switzerland\": \"cb4313847fec5d64e08e0549340a914da569db0f\",\n",
    "    \"Thun,Switzerland\": \"4e567234358d307fb77c5cb5514150df3cd59a3c\",\n",
    "    \"Verbier,Switzerland\": \"d9ab4f99f16929d6a5dea4a9b3f20d60af8dd3f9\",\n",
    "    \"Zurich,Switzerland\": \"be1ac363913afba07be684e70dcbb7b7dcfd2ba1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aee47f3-41ef-4091-88c7-f541c8cdc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_website_data(locations, year:int, month:int, day:int) -> pd.DataFrame:\n",
    "    \"\"\"Access the website with the needed parameters; return a PandasDataFrame\"\"\"\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for location, authority in locations.items():\n",
    "    \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            result = await client.post(\n",
    "                url=\"https://www.wetter2.com/v1/past-weather/\",\n",
    "                headers={\n",
    "                    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                    \"Authority\": authority,\n",
    "                },\n",
    "                data={\n",
    "                    \"place\": location,\n",
    "                    \"day\": day,\n",
    "                    \"month\": month,\n",
    "                    \"city\": location.split(',')[0].replace('_', ' '),\n",
    "                    \"country\": location.split(',')[1],\n",
    "                    \"language\": \"german\"\n",
    "                },\n",
    "            )\n",
    "            result.raise_for_status()\n",
    "            result_json = result.json()\n",
    "            result_years = result_json['data']['years']\n",
    "\n",
    "            # Error, if the result is not in the form of a dictionary\n",
    "            if not isinstance(result_years, dict):\n",
    "                raise ValueError(f\"Cannot parse data for {day=} {month=}: {str(res_years)[:50]}...\")\n",
    "\n",
    "            for k, v in result_years.items():\n",
    "                if int(k)!=year:\n",
    "                    continue\n",
    "                date = datetime.date(year=year, month=month, day=day)\n",
    "                if v.get(\"table\") is None:\n",
    "                    continue\n",
    "                res_table = v[\"table\"]\n",
    "    \n",
    "                soup = bs4.BeautifulSoup(res_table)\n",
    "                head = soup.table.thead\n",
    "    \n",
    "                # Create Index\n",
    "                timestamps = []\n",
    "                for td in head.find_all(\"td\"):\n",
    "                    dt = datetime.datetime.combine(date, datetime.time.fromisoformat(td.text))\n",
    "                    dt = pd.Timestamp(dt).tz_localize(\"UTC\")\n",
    "                    timestamps.append(dt)\n",
    "                index = pd.MultiIndex.from_frame(pd.DataFrame(data={\"location\": location, \"datetime\": timestamps}))\n",
    "    \n",
    "    \n",
    "                # Get the data of the html-body and create a dictionary with Temperature, Rain, Wind and Cloudiness\n",
    "                body = soup.table.tbody\n",
    "                data = dict(\n",
    "                    temp_C=[float(span[\"data-temp\"]) for span in body.find(\"th\", string=\"Temperatur\").parent.find_all(\"span\", class_=\"day_temp\")],\n",
    "                    rain_mm=[float(span[\"data-length\"]) for span in body.find(\"th\", string=\"Niederschlag\").parent.find_all(\"span\", attrs={\"data-length\": True})],\n",
    "                    wind_kmh=[float(span[\"data-wind\"]) for span in body.find(\"th\", string=\"Wind\").parent.find_all(\"span\", class_=\"day_wind\")],\n",
    "                    cloud_percent=[float(td.text.strip(\"%\")) for td in body.find(\"th\", string=\"Wolkendecke\").parent.find_all(\"td\")]                                         \n",
    "                )\n",
    "\n",
    "                result = pd.DataFrame(data=data, index=index)\n",
    "                results.append(result)\n",
    "\n",
    "\n",
    "    if results: \n",
    "        # Concate the list-entries to a Dataframe\n",
    "        return pd.concat(results)\n",
    "    \n",
    "    else:\n",
    "        # Return empty dataframe, if there is no data\n",
    "        data = dict(\n",
    "            location = pd.Series([], dtype=str),\n",
    "            datetime = pd.Series([], dtype=\"M8[ns]\"), # M8 is Timestamp\n",
    "            temp_C=pd.Series([], dtype=float),\n",
    "            rain_mm=pd.Series([], dtype=float),\n",
    "            wind_kmh=pd.Series([], dtype=float),\n",
    "            cloud_percent=pd.Series([], dtype=float),                                        \n",
    "        )\n",
    "        return pd.DataFrame(data=data).set_index([\"location\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e09b9d47-e477-4115-9d52-cb412362e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_db(collection, df):\n",
    "    \"\"\"Insert the data to the collection; if there is already a data-set with the same location and time,\n",
    "    an Error is raised, but the rest of the inserts will carry on\"\"\"\n",
    "\n",
    "    data = df.reset_index().to_dict(\"records\")\n",
    "\n",
    "    collection.insert_many(\n",
    "        data,\n",
    "        ordered=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3399520-98ae-4951-840a-e7d80f7b8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scraping():\n",
    "    \"\"\"Run the program: Scraping the website, Insert it to DB\"\"\"\n",
    "\n",
    "    collection = connect_to_db()\n",
    "    end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    start_date = end_date - datetime.timedelta(days=365)\n",
    "    \n",
    "    date = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    collected_dfs = []\n",
    "    \n",
    "    for d in date:\n",
    "        print(f'Working on {d.year}-{d.month}-{d.day}')\n",
    "        df = await scrape_website_data(locations=locations, year=d.year, month=d.month, day=d.day)\n",
    "        collected_dfs.append(df)\n",
    "            \n",
    "    df_to_insert = pd.concat(collected_dfs)  \n",
    "\n",
    "    print(\"all data scraped, ready to insert in db\")\n",
    "    \n",
    "    try:\n",
    "        insert_data_to_db(collection, df_to_insert)\n",
    "    except pymongo.errors.BulkWriteError as ex:\n",
    "        result = dict(ex.details)\n",
    "        write_errors = result.pop(\"writeErrors\",[])\n",
    "        ok = all(err.get(\"code\") == 11000 for err in write_errors)\n",
    "        ok = ok and not result.get(\"writeConcernErrors\")\n",
    "        n_success = result['nInserted']\n",
    "        n_duplicate = len(write_errors)\n",
    "        ok = ok and (n_success + n_duplicate) == df_to_insert.shape[0]\n",
    "        if ok:\n",
    "            print(f\"Discarded {n_duplicate} inserts due to duplicate keys, inserted {n_success} documents.\")\n",
    "        else:\n",
    "            had_write_concern = len(result.get(\"writeConcernErrors\",[]))\n",
    "            not_discarded = sum(err.get(\"code\") != 11000 for err in write_errors)\n",
    "            raise RuntimeError(f\"Unexpected error; {n_duplicate=} {n_success=} {df_to_insert.shape[0]=} {had_write_concern=} {not_discarded=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "693db817-407b-486d-9a6c-a5f5e396d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2023-3-11\n",
      "Working on 2023-3-12\n",
      "Working on 2023-3-13\n",
      "Working on 2023-3-14\n",
      "Working on 2023-3-15\n",
      "Working on 2023-3-16\n",
      "Working on 2023-3-17\n",
      "Working on 2023-3-18\n",
      "Working on 2023-3-19\n",
      "Working on 2023-3-20\n",
      "Working on 2023-3-21\n",
      "Working on 2023-3-22\n",
      "Working on 2023-3-23\n",
      "Working on 2023-3-24\n",
      "Working on 2023-3-25\n",
      "Working on 2023-3-26\n",
      "Working on 2023-3-27\n",
      "Working on 2023-3-28\n",
      "Working on 2023-3-29\n",
      "Working on 2023-3-30\n",
      "Working on 2023-3-31\n",
      "Working on 2023-4-1\n",
      "Working on 2023-4-2\n",
      "Working on 2023-4-3\n",
      "Working on 2023-4-4\n",
      "Working on 2023-4-5\n",
      "Working on 2023-4-6\n",
      "Working on 2023-4-7\n",
      "Working on 2023-4-8\n",
      "Working on 2023-4-9\n",
      "Working on 2023-4-10\n",
      "Working on 2023-4-11\n",
      "Working on 2023-4-12\n",
      "Working on 2023-4-13\n",
      "Working on 2023-4-14\n",
      "Working on 2023-4-15\n",
      "Working on 2023-4-16\n",
      "Working on 2023-4-17\n",
      "Working on 2023-4-18\n",
      "Working on 2023-4-19\n",
      "Working on 2023-4-20\n",
      "Working on 2023-4-21\n",
      "Working on 2023-4-22\n",
      "Working on 2023-4-23\n",
      "Working on 2023-4-24\n",
      "Working on 2023-4-25\n",
      "Working on 2023-4-26\n",
      "Working on 2023-4-27\n",
      "Working on 2023-4-28\n",
      "Working on 2023-4-29\n",
      "Working on 2023-4-30\n",
      "Working on 2023-5-1\n",
      "Working on 2023-5-2\n",
      "Working on 2023-5-3\n",
      "Working on 2023-5-4\n",
      "Working on 2023-5-5\n",
      "Working on 2023-5-6\n",
      "Working on 2023-5-7\n",
      "Working on 2023-5-8\n",
      "Working on 2023-5-9\n",
      "Working on 2023-5-10\n",
      "Working on 2023-5-11\n",
      "Working on 2023-5-12\n",
      "Working on 2023-5-13\n",
      "Working on 2023-5-14\n",
      "Working on 2023-5-15\n",
      "Working on 2023-5-16\n",
      "Working on 2023-5-17\n",
      "Working on 2023-5-18\n",
      "Working on 2023-5-19\n",
      "Working on 2023-5-20\n",
      "Working on 2023-5-21\n",
      "Working on 2023-5-22\n",
      "Working on 2023-5-23\n",
      "Working on 2023-5-24\n",
      "Working on 2023-5-25\n",
      "Working on 2023-5-26\n",
      "Working on 2023-5-27\n",
      "Working on 2023-5-28\n",
      "Working on 2023-5-29\n",
      "Working on 2023-5-30\n",
      "Working on 2023-5-31\n",
      "Working on 2023-6-1\n",
      "Working on 2023-6-2\n",
      "Working on 2023-6-3\n",
      "Working on 2023-6-4\n",
      "Working on 2023-6-5\n",
      "Working on 2023-6-6\n",
      "Working on 2023-6-7\n",
      "Working on 2023-6-8\n",
      "Working on 2023-6-9\n",
      "Working on 2023-6-10\n",
      "Working on 2023-6-11\n",
      "Working on 2023-6-12\n",
      "Working on 2023-6-13\n",
      "Working on 2023-6-14\n",
      "Working on 2023-6-15\n",
      "Working on 2023-6-16\n",
      "Working on 2023-6-17\n",
      "Working on 2023-6-18\n",
      "Working on 2023-6-19\n",
      "Working on 2023-6-20\n",
      "Working on 2023-6-21\n",
      "Working on 2023-6-22\n",
      "Working on 2023-6-23\n",
      "Working on 2023-6-24\n",
      "Working on 2023-6-25\n",
      "Working on 2023-6-26\n",
      "Working on 2023-6-27\n",
      "Working on 2023-6-28\n",
      "Working on 2023-6-29\n",
      "Working on 2023-6-30\n",
      "Working on 2023-7-1\n",
      "Working on 2023-7-2\n",
      "Working on 2023-7-3\n",
      "Working on 2023-7-4\n",
      "Working on 2023-7-5\n",
      "Working on 2023-7-6\n",
      "Working on 2023-7-7\n",
      "Working on 2023-7-8\n",
      "Working on 2023-7-9\n",
      "Working on 2023-7-10\n",
      "Working on 2023-7-11\n",
      "Working on 2023-7-12\n",
      "Working on 2023-7-13\n",
      "Working on 2023-7-14\n",
      "Working on 2023-7-15\n",
      "Working on 2023-7-16\n",
      "Working on 2023-7-17\n",
      "Working on 2023-7-18\n",
      "Working on 2023-7-19\n",
      "Working on 2023-7-20\n",
      "Working on 2023-7-21\n",
      "Working on 2023-7-22\n",
      "Working on 2023-7-23\n",
      "Working on 2023-7-24\n",
      "Working on 2023-7-25\n",
      "Working on 2023-7-26\n",
      "Working on 2023-7-27\n",
      "Working on 2023-7-28\n",
      "Working on 2023-7-29\n",
      "Working on 2023-7-30\n",
      "Working on 2023-7-31\n",
      "Working on 2023-8-1\n",
      "Working on 2023-8-2\n",
      "Working on 2023-8-3\n",
      "Working on 2023-8-4\n",
      "Working on 2023-8-5\n",
      "Working on 2023-8-6\n",
      "Working on 2023-8-7\n",
      "Working on 2023-8-8\n",
      "Working on 2023-8-9\n",
      "Working on 2023-8-10\n",
      "Working on 2023-8-11\n",
      "Working on 2023-8-12\n",
      "Working on 2023-8-13\n",
      "Working on 2023-8-14\n",
      "Working on 2023-8-15\n",
      "Working on 2023-8-16\n",
      "Working on 2023-8-17\n",
      "Working on 2023-8-18\n",
      "Working on 2023-8-19\n",
      "Working on 2023-8-20\n",
      "Working on 2023-8-21\n",
      "Working on 2023-8-22\n",
      "Working on 2023-8-23\n",
      "Working on 2023-8-24\n",
      "Working on 2023-8-25\n",
      "Working on 2023-8-26\n",
      "Working on 2023-8-27\n",
      "Working on 2023-8-28\n",
      "Working on 2023-8-29\n",
      "Working on 2023-8-30\n",
      "Working on 2023-8-31\n",
      "Working on 2023-9-1\n",
      "Working on 2023-9-2\n",
      "Working on 2023-9-3\n",
      "Working on 2023-9-4\n",
      "Working on 2023-9-5\n",
      "Working on 2023-9-6\n",
      "Working on 2023-9-7\n",
      "Working on 2023-9-8\n",
      "Working on 2023-9-9\n",
      "Working on 2023-9-10\n",
      "Working on 2023-9-11\n",
      "Working on 2023-9-12\n",
      "Working on 2023-9-13\n",
      "Working on 2023-9-14\n",
      "Working on 2023-9-15\n",
      "Working on 2023-9-16\n",
      "Working on 2023-9-17\n",
      "Working on 2023-9-18\n",
      "Working on 2023-9-19\n",
      "Working on 2023-9-20\n",
      "Working on 2023-9-21\n",
      "Working on 2023-9-22\n",
      "Working on 2023-9-23\n",
      "Working on 2023-9-24\n",
      "Working on 2023-9-25\n",
      "Working on 2023-9-26\n",
      "Working on 2023-9-27\n",
      "Working on 2023-9-28\n",
      "Working on 2023-9-29\n",
      "Working on 2023-9-30\n",
      "Working on 2023-10-1\n",
      "Working on 2023-10-2\n",
      "Working on 2023-10-3\n",
      "Working on 2023-10-4\n",
      "Working on 2023-10-5\n",
      "Working on 2023-10-6\n",
      "Working on 2023-10-7\n",
      "Working on 2023-10-8\n",
      "Working on 2023-10-9\n",
      "Working on 2023-10-10\n",
      "Working on 2023-10-11\n",
      "Working on 2023-10-12\n",
      "Working on 2023-10-13\n",
      "Working on 2023-10-14\n",
      "Working on 2023-10-15\n",
      "Working on 2023-10-16\n",
      "Working on 2023-10-17\n",
      "Working on 2023-10-18\n",
      "Working on 2023-10-19\n",
      "Working on 2023-10-20\n",
      "Working on 2023-10-21\n",
      "Working on 2023-10-22\n",
      "Working on 2023-10-23\n",
      "Working on 2023-10-24\n",
      "Working on 2023-10-25\n",
      "Working on 2023-10-26\n",
      "Working on 2023-10-27\n",
      "Working on 2023-10-28\n",
      "Working on 2023-10-29\n",
      "Working on 2023-10-30\n",
      "Working on 2023-10-31\n",
      "Working on 2023-11-1\n",
      "Working on 2023-11-2\n",
      "Working on 2023-11-3\n",
      "Working on 2023-11-4\n",
      "Working on 2023-11-5\n",
      "Working on 2023-11-6\n",
      "Working on 2023-11-7\n",
      "Working on 2023-11-8\n",
      "Working on 2023-11-9\n",
      "Working on 2023-11-10\n",
      "Working on 2023-11-11\n",
      "Working on 2023-11-12\n",
      "Working on 2023-11-13\n",
      "Working on 2023-11-14\n",
      "Working on 2023-11-15\n",
      "Working on 2023-11-16\n",
      "Working on 2023-11-17\n",
      "Working on 2023-11-18\n",
      "Working on 2023-11-19\n",
      "Working on 2023-11-20\n",
      "Working on 2023-11-21\n",
      "Working on 2023-11-22\n",
      "Working on 2023-11-23\n",
      "Working on 2023-11-24\n",
      "Working on 2023-11-25\n",
      "Working on 2023-11-26\n",
      "Working on 2023-11-27\n",
      "Working on 2023-11-28\n",
      "Working on 2023-11-29\n",
      "Working on 2023-11-30\n",
      "Working on 2023-12-1\n",
      "Working on 2023-12-2\n",
      "Working on 2023-12-3\n",
      "Working on 2023-12-4\n",
      "Working on 2023-12-5\n",
      "Working on 2023-12-6\n",
      "Working on 2023-12-7\n",
      "Working on 2023-12-8\n",
      "Working on 2023-12-9\n",
      "Working on 2023-12-10\n",
      "Working on 2023-12-11\n",
      "Working on 2023-12-12\n",
      "Working on 2023-12-13\n",
      "Working on 2023-12-14\n",
      "Working on 2023-12-15\n",
      "Working on 2023-12-16\n",
      "Working on 2023-12-17\n",
      "Working on 2023-12-18\n",
      "Working on 2023-12-19\n",
      "Working on 2023-12-20\n",
      "Working on 2023-12-21\n",
      "Working on 2023-12-22\n",
      "Working on 2023-12-23\n",
      "Working on 2023-12-24\n",
      "Working on 2023-12-25\n",
      "Working on 2023-12-26\n",
      "Working on 2023-12-27\n",
      "Working on 2023-12-28\n",
      "Working on 2023-12-29\n",
      "Working on 2023-12-30\n",
      "Working on 2023-12-31\n",
      "Working on 2024-1-1\n",
      "Working on 2024-1-2\n",
      "Working on 2024-1-3\n",
      "Working on 2024-1-4\n",
      "Working on 2024-1-5\n",
      "Working on 2024-1-6\n",
      "Working on 2024-1-7\n",
      "Working on 2024-1-8\n",
      "Working on 2024-1-9\n",
      "Working on 2024-1-10\n",
      "Working on 2024-1-11\n",
      "Working on 2024-1-12\n",
      "Working on 2024-1-13\n",
      "Working on 2024-1-14\n",
      "Working on 2024-1-15\n",
      "Working on 2024-1-16\n",
      "Working on 2024-1-17\n",
      "Working on 2024-1-18\n",
      "Working on 2024-1-19\n",
      "Working on 2024-1-20\n",
      "Working on 2024-1-21\n",
      "Working on 2024-1-22\n",
      "Working on 2024-1-23\n",
      "Working on 2024-1-24\n",
      "Working on 2024-1-25\n",
      "Working on 2024-1-26\n",
      "Working on 2024-1-27\n",
      "Working on 2024-1-28\n",
      "Working on 2024-1-29\n",
      "Working on 2024-1-30\n",
      "Working on 2024-1-31\n",
      "Working on 2024-2-1\n",
      "Working on 2024-2-2\n",
      "Working on 2024-2-3\n",
      "Working on 2024-2-4\n",
      "Working on 2024-2-5\n",
      "Working on 2024-2-6\n",
      "Working on 2024-2-7\n",
      "Working on 2024-2-8\n",
      "Working on 2024-2-9\n",
      "Working on 2024-2-10\n",
      "Working on 2024-2-11\n",
      "Working on 2024-2-12\n",
      "Working on 2024-2-13\n",
      "Working on 2024-2-14\n",
      "Working on 2024-2-15\n",
      "Working on 2024-2-16\n",
      "Working on 2024-2-17\n",
      "Working on 2024-2-18\n",
      "Working on 2024-2-19\n",
      "Working on 2024-2-20\n",
      "Working on 2024-2-21\n",
      "Working on 2024-2-22\n",
      "Working on 2024-2-23\n",
      "Working on 2024-2-24\n",
      "Working on 2024-2-25\n",
      "Working on 2024-2-26\n",
      "Working on 2024-2-27\n",
      "Working on 2024-2-28\n",
      "Working on 2024-2-29\n",
      "Working on 2024-3-1\n",
      "Working on 2024-3-2\n",
      "Working on 2024-3-3\n",
      "Working on 2024-3-4\n",
      "Working on 2024-3-5\n",
      "Working on 2024-3-6\n",
      "Working on 2024-3-7\n",
      "Working on 2024-3-8\n",
      "Working on 2024-3-9\n",
      "Working on 2024-3-10\n",
      "all data scraped, ready to insert in db\n",
      "Discarded 3456 inserts due to duplicate keys, inserted 308100 documents.\n"
     ]
    }
   ],
   "source": [
    "await scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528813a-7593-4751-98fc-3882d1c31fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
