{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59665da-e6c8-44b2-9f5a-db28f1eb4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "import pymongo\n",
    "import httpx\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b86bb5-9c9d-41f8-935d-954fb88ec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get MongoDB-URI\n",
    "    mongodb_uri = os.getenv(\"MONGODB_URI\")\n",
    "    DBclient = pymongo.MongoClient(mongodb_uri)\n",
    "    db = DBclient[\"MDM-Python-MeinProjekt\"]\n",
    "\n",
    "    if \"Wetter\" in db.list_collection_names():\n",
    "        return db[\"Wetter\"]\n",
    "    else:\n",
    "        collection = db[\"Wetter\"]\n",
    "        collection.create_index([\n",
    "            (\"location\", pymongo.ASCENDING),\n",
    "            (\"datetime\", pymongo.ASCENDING),\n",
    "        ], unique=True)\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65a990d-3cce-481c-bdc3-a34430f474aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36 Locations in Switzerland\n",
    "locations = {\n",
    "    \"Aarau,Switzerland\": \"5a5ecc58df562835e3fcbae5f8c52e64e7247918\",\n",
    "    \"Appenzell,Switzerland\": \"162a2967db482d9de5e1db7b98c7fa5a779f2875\",\n",
    "    \"Basel,Switzerland\": \"e502a0a8fc421e6a8f9f1c4034f6b46ff6f59f62\",\n",
    "    \"Bellinzona,Switzerland\": \"f398035eff9921de0368cc494578468b1d5c99ad\",\n",
    "    \"Bern,Switzerland\": \"94fa2a5396a4723b31142ab413d3ec1be77d62d8\",\n",
    "    \"Chur,Switzerland\": \"6c0da286ee836415b66b138d6f13076fbcdb3899\",\n",
    "    \"Davos,Switzerland\": \"e260b1b791f27abac6a4f7771a2401be6aee67a9\",\n",
    "    \"Delemont,Switzerland\": \"f661d8d34fbb2431d6f02a9613c09a3782655d55\",\n",
    "    \"Einsiedeln,Switzerland\": \"14ee1f6a8ff571616de7f378af94b0a588c67bda\",\n",
    "    \"Frauenfeld,Switzerland\": \"8bf573628b0fd96382f03bef84c063f3aa481e21\",\n",
    "    \"Fribourg,Switzerland\": \"f5240fc4fcffbe2c1680ee6f026349a7c2c0da48\",\n",
    "    \"Geneva,Switzerland\": \"eacb20159164fa15c55f87d7d66068b4b2ceaf39\",\n",
    "    \"Glarus,Switzerland\": \"dcb4a71fe438a4165ee26a4d370d259a2c5896d0\",\n",
    "    \"Grindelwald,Switzerland\": \"ab1c057337ed2fca115ed0e16b4ec2467466aaad\",\n",
    "    \"La_Chaux_De_Fonds,Switzerland\": \"d78dc7da556213f33175d58cc6d1de8534ac3a6c\",\n",
    "    \"Laax,Switzerland\": \"eb8dfc2322338b344ef7f8f1063e01f91b555137\",\n",
    "    \"Lausanne,Switzerland\": \"fb15d1e5d748ce024a944f59f9bf651919032bd1\",\n",
    "    \"Lauterbrunnen,Switzerland\": \"8a9e5db01b10728c85ac4944d19e79e515f3deba\",\n",
    "    \"Lenzerheide,Switzerland\": \"73330944ee4dbe329e5f8cfbf7bc49a0f420ba2f\",\n",
    "    \"Leukerbad,Switzerland\": \"7d2f2e013101f38faf031a38c5d9b348aeb883e3\",\n",
    "    \"Lucerne,Switzerland\": \"470964fac0430b6083c52ddd3e2a400c542e0e60\",\n",
    "    \"Lugano,Switzerland\": \"f9bc6e13323ac7547a608fa227ff7e274284d1f2\",\n",
    "    \"Montreux,Switzerland\": \"0034c59b5b977acd17fe5837e5845ae9f41e5a09\",\n",
    "    \"Neuchatel,Switzerland\": \"684590561854da6d27a36d9e659cc4739f675b1c\",\n",
    "    \"Romanshorn,Switzerland\": \"39fe927062dc9f07e6b3abed0189caf29b9745fd\",\n",
    "    \"Saas_Fee,Switzerland\": \"865c64f03112f377ad70f301a218110eefa322b8\",\n",
    "    \"Sarnen,Switzerland\": \"8a7c9fe3ae4cbce26ca0d3447542641f60ef781c\",\n",
    "    \"Savognin,Switzerland\": \"8bb604df8ccd428e2f37e5c4239c93f4dc55f19f\",\n",
    "    \"Schaffhausen,Switzerland\": \"8b5f38ff71ef8dcc2b857f39361149bb6193e4c3\",\n",
    "    \"Sion,Switzerland\": \"f092059ab917cc1a9a335215a1f4744944feaec3\",\n",
    "    \"Solothurn,Switzerland\": \"a6e3ca1b9fedf679fcc41159f5f5a56a58ca7354\",\n",
    "    \"St_Gallen,Switzerland\": \"c6a7895be45659cd932d951b975b522d5964f9af\",\n",
    "    \"Tenero,Switzerland\": \"cb4313847fec5d64e08e0549340a914da569db0f\",\n",
    "    \"Thun,Switzerland\": \"4e567234358d307fb77c5cb5514150df3cd59a3c\",\n",
    "    \"Verbier,Switzerland\": \"d9ab4f99f16929d6a5dea4a9b3f20d60af8dd3f9\",\n",
    "    \"Zurich,Switzerland\": \"be1ac363913afba07be684e70dcbb7b7dcfd2ba1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aee47f3-41ef-4091-88c7-f541c8cdc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_website_data(locations, year:int, month:int, day:int) -> pd.DataFrame:\n",
    "    \"\"\"Access the website with the needed parameters; return a PandasDataFrame\"\"\"\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for location, authority in locations.items():\n",
    "    \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            result = await client.post(\n",
    "                url=\"https://www.wetter2.com/v1/past-weather/\",\n",
    "                headers={\n",
    "                    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                    \"Authority\": authority,\n",
    "                },\n",
    "                data={\n",
    "                    \"place\": location,\n",
    "                    \"day\": day,\n",
    "                    \"month\": month,\n",
    "                    \"city\": location.split(',')[0].replace('_', ' '),\n",
    "                    \"country\": location.split(',')[1],\n",
    "                    \"language\": \"german\"\n",
    "                },\n",
    "            )\n",
    "            result.raise_for_status()\n",
    "            result_json = result.json()\n",
    "            result_years = result_json['data']['years']\n",
    "\n",
    "            # Error, if the result is not in the form of a dictionary\n",
    "            if not isinstance(result_years, dict):\n",
    "                raise ValueError(f\"Cannot parse data for {day=} {month=}: {str(res_years)[:50]}...\")\n",
    "\n",
    "            for k, v in result_years.items():\n",
    "                if int(k)!=year:\n",
    "                    continue\n",
    "                date = datetime.date(year=year, month=month, day=day)\n",
    "                if v.get(\"table\") is None:\n",
    "                    continue\n",
    "                res_table = v[\"table\"]\n",
    "    \n",
    "                soup = bs4.BeautifulSoup(res_table)\n",
    "                head = soup.table.thead\n",
    "    \n",
    "                # Create Index\n",
    "                timestamps = []\n",
    "                for td in head.find_all(\"td\"):\n",
    "                    dt = datetime.datetime.combine(date, datetime.time.fromisoformat(td.text))\n",
    "                    dt = pd.Timestamp(dt).tz_localize(\"UTC\")\n",
    "                    timestamps.append(dt)\n",
    "                index = pd.MultiIndex.from_frame(pd.DataFrame(data={\"location\": location, \"datetime\": timestamps}))\n",
    "    \n",
    "    \n",
    "                # Get the data of the html-body and create a dictionary with Temperature, Rain, Wind and Cloudiness\n",
    "                body = soup.table.tbody\n",
    "                data = dict(\n",
    "                    temp_C=[float(span[\"data-temp\"]) for span in body.find(\"th\", string=\"Temperatur\").parent.find_all(\"span\", class_=\"day_temp\")],\n",
    "                    rain_mm=[float(span[\"data-length\"]) for span in body.find(\"th\", string=\"Niederschlag\").parent.find_all(\"span\", attrs={\"data-length\": True})],\n",
    "                    wind_kmh=[float(span[\"data-wind\"]) for span in body.find(\"th\", string=\"Wind\").parent.find_all(\"span\", class_=\"day_wind\")],\n",
    "                    cloud_percent=[float(td.text.strip(\"%\")) for td in body.find(\"th\", string=\"Wolkendecke\").parent.find_all(\"td\")]                                         \n",
    "                )\n",
    "\n",
    "                result = pd.DataFrame(data=data, index=index)\n",
    "                results.append(result)\n",
    "\n",
    "\n",
    "    if results: \n",
    "        # Concate the list-entries to a Dataframe\n",
    "        return pd.concat(results)\n",
    "    \n",
    "    else:\n",
    "        # Return empty dataframe, if there is no data\n",
    "        data = dict(\n",
    "            location = pd.Series([], dtype=str),\n",
    "            datetime = pd.Series([], dtype=\"M8[ns]\"), # M8 is Timestamp\n",
    "            temp_C=pd.Series([], dtype=float),\n",
    "            rain_mm=pd.Series([], dtype=float),\n",
    "            wind_kmh=pd.Series([], dtype=float),\n",
    "            cloud_percent=pd.Series([], dtype=float),                                        \n",
    "        )\n",
    "        return pd.DataFrame(data=data).set_index([\"location\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09b9d47-e477-4115-9d52-cb412362e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_db(collection, df):\n",
    "    \"\"\"Insert the data to the collection; if there is already a data-set with the same location and time,\n",
    "    an Error is raised, but the rest of the inserts will carry on\"\"\"\n",
    "\n",
    "    data = df.reset_index().to_dict(\"records\")\n",
    "\n",
    "    collection.insert_many(\n",
    "        data,\n",
    "        ordered=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3399520-98ae-4951-840a-e7d80f7b8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scraping():\n",
    "    \"\"\"Run the program: Scraping the website, Insert it to DB\"\"\"\n",
    "\n",
    "    collection = connect_to_db()\n",
    "    end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    start_date = end_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    date = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    collected_dfs = []\n",
    "    \n",
    "    for d in date:\n",
    "        print(f'Working on {d.year}-{d.month}-{d.day}')\n",
    "        df = await scrape_website_data(locations=locations, year=d.year, month=d.month, day=d.day)\n",
    "        collected_dfs.append(df)\n",
    "            \n",
    "    df_to_insert = pd.concat(collected_dfs)  \n",
    "\n",
    "    print(\"all data scraped, ready to insert in db\")\n",
    "    \n",
    "    try:\n",
    "        insert_data_to_db(collection, df_to_insert)\n",
    "    except pymongo.errors.BulkWriteError as ex:\n",
    "        result = dict(ex.details)\n",
    "        write_errors = result.pop(\"writeErrors\",[])\n",
    "        ok = all(err.get(\"code\") == 11000 for err in write_errors)\n",
    "        ok = ok and not result.get(\"writeConcernErrors\")\n",
    "        n_success = result['nInserted']\n",
    "        n_duplicate = len(write_errors)\n",
    "        ok = ok and (n_success + n_duplicate) == df_to_insert.shape[0]\n",
    "        if ok:\n",
    "            print(f\"Discarded {n_duplicate} inserts due to duplicate keys, inserted {n_success} documents.\")\n",
    "        else:\n",
    "            had_write_concern = len(result.get(\"writeConcernErrors\",[]))\n",
    "            not_discarded = sum(err.get(\"code\") != 11000 for err in write_errors)\n",
    "            raise RuntimeError(f\"Unexpected error; {n_duplicate=} {n_success=} {df_to_insert.shape[0]=} {had_write_concern=} {not_discarded=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693db817-407b-486d-9a6c-a5f5e396d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
