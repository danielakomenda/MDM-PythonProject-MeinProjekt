{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59665da-e6c8-44b2-9f5a-db28f1eb4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "import pymongo\n",
    "import httpx\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b86bb5-9c9d-41f8-935d-954fb88ec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get MongoDB-URI\n",
    "    mongodb_uri = os.getenv(\"MONGODB_URI\")\n",
    "    DBclient = pymongo.MongoClient(mongodb_uri)\n",
    "    db = DBclient[\"MDM-Python-MeinProjekt\"]\n",
    "\n",
    "    if \"Wetter\" in db.list_collection_names():\n",
    "        return db[\"Wetter\"]\n",
    "    else:\n",
    "        collection = db[\"Wetter\"]\n",
    "        collection.create_index([\n",
    "            (\"location\", pymongo.ASCENDING),\n",
    "            (\"datetime\", pymongo.ASCENDING),\n",
    "        ], unique=True)\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65a990d-3cce-481c-bdc3-a34430f474aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47 locations in Switzerland\n",
    "locations = {\n",
    "    \"Aarau,Switzerland\": \"5a5ecc58df562835e3fcbae5f8c52e64e7247918\",\n",
    "    \"Amriswil,Switzerland\": \"3d2affaab700ae926e9e7daf045849e1e03ba0c3\",\n",
    "    \"Andermatt,Switzerland\": \"4fa4d953b6595a92f1c013fc4edd06da171b87a6\",\n",
    "    \"Appenzell,Switzerland\": \"162a2967db482d9de5e1db7b98c7fa5a779f2875\",\n",
    "    \"Basel,Switzerland\": \"e502a0a8fc421e6a8f9f1c4034f6b46ff6f59f62\",\n",
    "    \"Bellinzona,Switzerland\": \"f398035eff9921de0368cc494578468b1d5c99ad\",\n",
    "    \"Bern,Switzerland\": \"94fa2a5396a4723b31142ab413d3ec1be77d62d8\",\n",
    "    \"Biel,Switzerland\": \"a6d41fac1a5c23907b50d10a3c2610ffba7e63ed\",\n",
    "    \"Chur,Switzerland\": \"6c0da286ee836415b66b138d6f13076fbcdb3899\",\n",
    "    \"Davos,Switzerland\": \"e260b1b791f27abac6a4f7771a2401be6aee67a9\",\n",
    "    \"Delemont,Switzerland\": \"f661d8d34fbb2431d6f02a9613c09a3782655d55\",\n",
    "    \"Einsiedeln,Switzerland\": \"14ee1f6a8ff571616de7f378af94b0a588c67bda\",\n",
    "    \"Frauenfeld,Switzerland\": \"8bf573628b0fd96382f03bef84c063f3aa481e21\",\n",
    "    \"Fribourg,Switzerland\": \"f5240fc4fcffbe2c1680ee6f026349a7c2c0da48\",\n",
    "    \"Geneva,Switzerland\": \"eacb20159164fa15c55f87d7d66068b4b2ceaf39\",\n",
    "    \"Glarus,Switzerland\": \"dcb4a71fe438a4165ee26a4d370d259a2c5896d0\",\n",
    "    \"Grindelwald,Switzerland\": \"ab1c057337ed2fca115ed0e16b4ec2467466aaad\",\n",
    "    \"Interlaken,Switzerland\": \"79dd56d026119e05406ee521078bf148ca22d7ca\",\n",
    "    \"La_Chaux_De_Fonds,Switzerland\": \"d78dc7da556213f33175d58cc6d1de8534ac3a6c\",\n",
    "    \"Laax,Switzerland\": \"eb8dfc2322338b344ef7f8f1063e01f91b555137\",\n",
    "    \"Lausanne,Switzerland\": \"fb15d1e5d748ce024a944f59f9bf651919032bd1\",\n",
    "    \"Lauterbrunnen,Switzerland\": \"8a9e5db01b10728c85ac4944d19e79e515f3deba\",\n",
    "    \"Lenzerheide,Switzerland\": \"73330944ee4dbe329e5f8cfbf7bc49a0f420ba2f\",\n",
    "    \"Leukerbad,Switzerland\": \"7d2f2e013101f38faf031a38c5d9b348aeb883e3\",\n",
    "    \"Lucerne,Switzerland\": \"470964fac0430b6083c52ddd3e2a400c542e0e60\",\n",
    "    \"Lugano,Switzerland\": \"f9bc6e13323ac7547a608fa227ff7e274284d1f2\",\n",
    "    \"Montreux,Switzerland\": \"0034c59b5b977acd17fe5837e5845ae9f41e5a09\",\n",
    "    \"Murten,Switzerland\": \"6eb04bd95e05c180215293c5aaaa2cf5743c1c5b\",\n",
    "    \"Neuchatel,Switzerland\": \"684590561854da6d27a36d9e659cc4739f675b1c\",\n",
    "    \"Payerne,Switzerland\": \"e1a073c91cf80e72b3535b2d1b274ec274f1a3e3\",\n",
    "    \"Pontresina,Switzerland\": \"e8f8d4d08d7cd2a4121fc88d0030d26fd98300de\",\n",
    "    \"Rapperswil,Switzerland\": \"e296f4a4b599b329b9472c1a785c634631ca1006\",\n",
    "    \"Romanshorn,Switzerland\": \"39fe927062dc9f07e6b3abed0189caf29b9745fd\",\n",
    "    \"Saas_Fee,Switzerland\": \"865c64f03112f377ad70f301a218110eefa322b8\",\n",
    "    \"Sarnen,Switzerland\": \"8a7c9fe3ae4cbce26ca0d3447542641f60ef781c\",\n",
    "    \"Savognin,Switzerland\": \"8bb604df8ccd428e2f37e5c4239c93f4dc55f19f\",\n",
    "    \"Schaffhausen,Switzerland\": \"8b5f38ff71ef8dcc2b857f39361149bb6193e4c3\",\n",
    "    \"Schwarzsee,Switzerland\": \"3bb733057180992f95dac64313f95ba8f6008e98\",\n",
    "    \"Sion,Switzerland\": \"f092059ab917cc1a9a335215a1f4744944feaec3\",\n",
    "    \"Solothurn,Switzerland\": \"a6e3ca1b9fedf679fcc41159f5f5a56a58ca7354\",\n",
    "    \"St_Gallen,Switzerland\": \"c6a7895be45659cd932d951b975b522d5964f9af\",\n",
    "    \"Tenero,Switzerland\": \"cb4313847fec5d64e08e0549340a914da569db0f\",\n",
    "    \"Thun,Switzerland\": \"4e567234358d307fb77c5cb5514150df3cd59a3c\",\n",
    "    \"Verbier,Switzerland\": \"d9ab4f99f16929d6a5dea4a9b3f20d60af8dd3f9\",\n",
    "    \"Vevey,Switzerland\": \"19283b9186e4a90e95159fa3c857e78aae5eef6d\",\n",
    "    \"Zermatt,Switzerland\": \"b165a59a6f441aa227744995430abf0d32530c3b\",\n",
    "    \"Zurich,Switzerland\": \"be1ac363913afba07be684e70dcbb7b7dcfd2ba1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aee47f3-41ef-4091-88c7-f541c8cdc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_website_data(locations, year:int, month:int, day:int) -> pd.DataFrame:\n",
    "    \"\"\"Access the website with the needed parameters; return a PandasDataFrame\"\"\"\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for location, authority in locations.items():\n",
    "    \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            result = await client.post(\n",
    "                url=\"https://www.wetter2.com/v1/past-weather/\",\n",
    "                headers={\n",
    "                    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                    \"Authority\": authority,\n",
    "                },\n",
    "                data={\n",
    "                    \"place\": location,\n",
    "                    \"day\": day,\n",
    "                    \"month\": month,\n",
    "                    \"city\": location.split(',')[0].replace('_', ' '),\n",
    "                    \"country\": location.split(',')[1],\n",
    "                    \"language\": \"german\"\n",
    "                },\n",
    "            )\n",
    "            result.raise_for_status()\n",
    "            result_json = result.json()\n",
    "            result_years = result_json['data']['years']\n",
    "\n",
    "            # Error, if the result is not in the form of a dictionary\n",
    "            if not isinstance(result_years, dict):\n",
    "                raise ValueError(f\"Cannot parse data for {day=} {month=}: {str(res_years)[:50]}...\")\n",
    "\n",
    "            for k, v in result_years.items():\n",
    "                if int(k)!=year:\n",
    "                    continue\n",
    "                date = datetime.date(year=year, month=month, day=day)\n",
    "                if v.get(\"table\") is None:\n",
    "                    continue\n",
    "                res_table = v[\"table\"]\n",
    "    \n",
    "                soup = bs4.BeautifulSoup(res_table)\n",
    "                head = soup.table.thead\n",
    "    \n",
    "                # Create Index\n",
    "                timestamps = []\n",
    "                for td in head.find_all(\"td\"):\n",
    "                    dt = datetime.datetime.combine(date, datetime.time.fromisoformat(td.text))\n",
    "                    dt = pd.Timestamp(dt).tz_localize(\"UTC\")\n",
    "                    timestamps.append(dt)\n",
    "                index = pd.MultiIndex.from_frame(pd.DataFrame(data={\"location\": location, \"datetime\": timestamps}))\n",
    "    \n",
    "    \n",
    "                # Get the data of the html-body and create a dictionary with Temperature, Rain, Wind and Cloudiness\n",
    "                body = soup.table.tbody\n",
    "                data = dict(\n",
    "                    temp_C=[float(span[\"data-temp\"]) for span in body.find(\"th\", string=\"Temperatur\").parent.find_all(\"span\", class_=\"day_temp\")],\n",
    "                    rain_mm=[float(span[\"data-length\"]) for span in body.find(\"th\", string=\"Niederschlag\").parent.find_all(\"span\", attrs={\"data-length\": True})],\n",
    "                    wind_kmh=[float(span[\"data-wind\"]) for span in body.find(\"th\", string=\"Wind\").parent.find_all(\"span\", class_=\"day_wind\")],\n",
    "                    cloud_percent=[float(td.text.strip(\"%\")) for td in body.find(\"th\", string=\"Wolkendecke\").parent.find_all(\"td\")]                                         \n",
    "                )\n",
    "\n",
    "                result = pd.DataFrame(data=data, index=index)\n",
    "                results.append(result)\n",
    "\n",
    "\n",
    "    if results: \n",
    "        # Concate the list-entries to a Dataframe\n",
    "        return pd.concat(results)\n",
    "    \n",
    "    else:\n",
    "        # Return empty dataframe, if there is no data\n",
    "        data = dict(\n",
    "            location = pd.Series([], dtype=str),\n",
    "            datetime = pd.Series([], dtype=\"M8[ns]\"), # M8 is Timestamp\n",
    "            temp_C=pd.Series([], dtype=float),\n",
    "            rain_mm=pd.Series([], dtype=float),\n",
    "            wind_kmh=pd.Series([], dtype=float),\n",
    "            cloud_percent=pd.Series([], dtype=float),                                        \n",
    "        )\n",
    "        return pd.DataFrame(data=data).set_index([\"location\", \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e09b9d47-e477-4115-9d52-cb412362e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_db(collection, df):\n",
    "    \"\"\"Insert the data to the collection; if there is already a data-set with the same location and time,\n",
    "    an Error is raised, but the rest of the inserts will carry on\"\"\"\n",
    "\n",
    "    data = df.reset_index().to_dict(\"records\")\n",
    "\n",
    "    collection.insert_many(\n",
    "        data,\n",
    "        ordered=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3399520-98ae-4951-840a-e7d80f7b8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scraping(locations):\n",
    "    \"\"\"Run the program: Scraping the website, Insert it to DB\"\"\"\n",
    "\n",
    "    collection = connect_to_db()\n",
    "    end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    start_date = end_date - datetime.timedelta(days=3)\n",
    "    \n",
    "    date = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    collected_dfs = []\n",
    "    \n",
    "    for d in date:\n",
    "        print(f'Working on {d.year}-{d.month}-{d.day}')\n",
    "        df = await scrape_website_data(locations=locations, year=d.year, month=d.month, day=d.day)\n",
    "        collected_dfs.append(df)\n",
    "            \n",
    "    df_to_insert = pd.concat(collected_dfs)  \n",
    "\n",
    "    print(\"all data scraped, ready to insert in db\")\n",
    "    insert_data_to_db(collection, df_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98764a1-70b4-462a-ac96-0d149a5009a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2022-12-16\n",
      "Working on 2022-12-17\n",
      "Working on 2022-12-18\n",
      "Working on 2022-12-19\n",
      "Working on 2022-12-20\n",
      "Working on 2022-12-21\n",
      "Working on 2022-12-22\n",
      "Working on 2022-12-23\n",
      "Working on 2022-12-24\n",
      "Working on 2022-12-25\n",
      "Working on 2022-12-26\n",
      "Working on 2022-12-27\n",
      "Working on 2022-12-28\n",
      "Working on 2022-12-29\n",
      "Working on 2022-12-30\n",
      "Working on 2022-12-31\n",
      "Working on 2023-1-1\n",
      "Working on 2023-1-2\n",
      "Working on 2023-1-3\n",
      "Working on 2023-1-4\n",
      "Working on 2023-1-5\n",
      "Working on 2023-1-6\n",
      "Working on 2023-1-7\n",
      "Working on 2023-1-8\n",
      "Working on 2023-1-9\n",
      "Working on 2023-1-10\n",
      "Working on 2023-1-11\n",
      "Working on 2023-1-12\n",
      "Working on 2023-1-13\n",
      "Working on 2023-1-14\n",
      "Working on 2023-1-15\n",
      "Working on 2023-1-16\n",
      "Working on 2023-1-17\n",
      "Working on 2023-1-18\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "hour must be in 0..23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scraping(locations)\n",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mscraping\u001b[0;34m(locations)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m date:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m.\u001b[39myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m.\u001b[39mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;241m.\u001b[39mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_website_data(locations\u001b[38;5;241m=\u001b[39mlocations, year\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39myear, month\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mmonth, day\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mday)\n\u001b[1;32m     15\u001b[0m     collected_dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     17\u001b[0m df_to_insert \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(collected_dfs)  \n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mscrape_website_data\u001b[0;34m(locations, year, month, day)\u001b[0m\n\u001b[1;32m     44\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m td \u001b[38;5;129;01min\u001b[39;00m head\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mcombine(date, \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromisoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     dt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(dt)\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     timestamps\u001b[38;5;241m.\u001b[39mappend(dt)\n",
      "\u001b[0;31mValueError\u001b[0m: hour must be in 0..23"
     ]
    }
   ],
   "source": [
    "await scraping(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bb0ac-e863-4a81-8e84-3a12a8e39889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
