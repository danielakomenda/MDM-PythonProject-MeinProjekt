{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59665da-e6c8-44b2-9f5a-db28f1eb4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import os\n",
    "import dotenv\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pymongo\n",
    "import httpx\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b86bb5-9c9d-41f8-935d-954fb88ec438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db():\n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get MongoDB-URI\n",
    "    mongodb_uri = os.getenv(\"MONGODB_URI\")\n",
    "    DBclient = pymongo.MongoClient(mongodb_uri)\n",
    "    db = DBclient[\"MDM-Python-MeinProjekt\"]\n",
    "\n",
    "    if \"Energie\" in db.list_collection_names():\n",
    "        return db[\"Energie\"]\n",
    "    else:\n",
    "        collection = db[\"Energie\"]\n",
    "        collection.create_index([\n",
    "            (\"country\", pymongo.ASCENDING),\n",
    "            (\"datetime\", pymongo.ASCENDING),\n",
    "        ], unique=True)\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aee47f3-41ef-4091-88c7-f541c8cdc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_website_data(country, date) -> pd.DataFrame:\n",
    "    \"\"\"Access the website with the needed parameters; return a PandasDataFrame\"\"\"\n",
    "    \n",
    "    # Create List with all 20 Productions-Types\n",
    "    productiontypes = [\n",
    "        (\"productionType.values\", f\"B{k:02}\") for k in range(1, 21)\n",
    "    ]\n",
    "    \n",
    "        \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        result = await client.get(\n",
    "            url=\"https://transparency.entsoe.eu/generation/r2/actualGenerationPerProductionType/show\",\n",
    "            headers={\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            },\n",
    "            params=list(\n",
    "                {\n",
    "                    \"viewType\": \"GRAPH\",\n",
    "                    \"areaType\": \"CTY\",\n",
    "                    \"dateTime.dateTime\": f\"{date:%d.%m.%Y} 00:00|UTC|DAYTIMERANGE\",\n",
    "                    \"dateTime.endDateTime\": f\"{date:%d.%m.%Y} 00:00|UTC|DAYTIMERANGE\",\n",
    "                    \"dateTime.timezone\": \"UTC\",\n",
    "                    \"area.values\": f\"CTY|{country}!CTY|{country}\",\n",
    "                }.items()) + productiontypes,\n",
    "            )\n",
    "            \n",
    "    # make sure the content is UTF-8 and parse the content with bs4\n",
    "    assert result.headers[\"content-type\"] == \"text/html;charset=UTF-8\", result.headers[\"content-type\"]\n",
    "    soup = bs4.BeautifulSoup(result.content.decode(\"utf-8\"))\n",
    "    \n",
    "    # select only the part 'script' and the chart-list of the http-file\n",
    "    javascript_str = soup.find(\"script\").text\n",
    "    match = re.search(r\"var\\s+chart\\s*=\\s*({.*})\\s*;\", javascript_str, re.S)\n",
    "    assert match is not None\n",
    "    \n",
    "    # returns the first element of the group\n",
    "    data = json.loads(match.group(1))\n",
    "    \n",
    "    # defines the columns for the dataframe\n",
    "    columns = {\n",
    "        k: \" \".join(v[\"title\"].split())\n",
    "        for k, v in\n",
    "        data[\"graphDesign\"].items()\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        data[\"chartData\"]\n",
    "    ).set_index(data[\"categoryName\"]).astype(float).rename(columns=columns)\n",
    "    \n",
    "    # combine time with date to get a real timestamp\n",
    "    df = df.set_index(pd.MultiIndex.from_arrays(\n",
    "        [\n",
    "            [country]*df.shape[0],\n",
    "            df.index.to_series().apply(\n",
    "                lambda v: datetime.datetime.combine(date, datetime.time.fromisoformat(v))\n",
    "            ).dt.tz_localize(\"UTC\"),\n",
    "        ],\n",
    "        names=[\"country\", \"datetime\"],\n",
    "    ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09b9d47-e477-4115-9d52-cb412362e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_db(collection, df):\n",
    "    \"\"\"Insert the data to the collection; if there is already a data-set with the same location and time,\n",
    "    an Error is raised, but the rest of the inserts will carry on\"\"\"\n",
    "\n",
    "    data = df.reset_index().to_dict(\"records\")\n",
    "\n",
    "    collection.insert_many(\n",
    "        data,\n",
    "        ordered=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3399520-98ae-4951-840a-e7d80f7b8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scraping():\n",
    "    \"\"\"Run the program: Scraping the website, Insert it to DB\"\"\"\n",
    "\n",
    "    collection = connect_to_db()\n",
    "    end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    start_date = end_date - datetime.timedelta(days=365)\n",
    "    country = \"10YCH-SWISSGRIDZ\"\n",
    "    \n",
    "    date = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    collected_dfs = []\n",
    "    \n",
    "    for d in date:\n",
    "        print(f'Working on {d.year}-{d.month}-{d.day}')\n",
    "        df = await scrape_website_data(country=country, date=d)\n",
    "        collected_dfs.append(df)\n",
    "            \n",
    "    df_to_insert = pd.concat(collected_dfs)  \n",
    "\n",
    "    print(\"all data scraped, ready to insert in db\")\n",
    "    \n",
    "    try:\n",
    "        insert_data_to_db(collection, df_to_insert)\n",
    "    except pymongo.errors.BulkWriteError as ex:\n",
    "        result = dict(ex.details)\n",
    "        write_errors = result.pop(\"writeErrors\",[])\n",
    "        ok = all(err.get(\"code\") == 11000 for err in write_errors)\n",
    "        ok = ok and not result.get(\"writeConcernErrors\")\n",
    "        n_success = result['nInserted']\n",
    "        n_duplicate = len(write_errors)\n",
    "        ok = ok and (n_success + n_duplicate) == df_to_insert.shape[0]\n",
    "        if ok:\n",
    "            print(f\"Discarded {n_duplicate} inserts due to duplicate keys, inserted {n_success} documents.\")\n",
    "        else:\n",
    "            had_write_concern = len(result.get(\"writeConcernErrors\",[]))\n",
    "            not_discarded = sum(err.get(\"code\") != 11000 for err in write_errors)\n",
    "            raise RuntimeError(f\"Unexpected error; {n_duplicate=} {n_success=} {df_to_insert.shape[0]=} {had_write_concern=} {not_discarded=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98764a1-70b4-462a-ac96-0d149a5009a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2023-3-10\n",
      "Working on 2023-3-11\n",
      "Working on 2023-3-12\n",
      "Working on 2023-3-13\n",
      "Working on 2023-3-14\n",
      "Working on 2023-3-15\n",
      "Working on 2023-3-16\n",
      "Working on 2023-3-17\n",
      "Working on 2023-3-18\n",
      "Working on 2023-3-19\n",
      "Working on 2023-3-20\n",
      "Working on 2023-3-21\n",
      "Working on 2023-3-22\n",
      "Working on 2023-3-23\n",
      "Working on 2023-3-24\n",
      "Working on 2023-3-25\n",
      "Working on 2023-3-26\n",
      "Working on 2023-3-27\n",
      "Working on 2023-3-28\n",
      "Working on 2023-3-29\n",
      "Working on 2023-3-30\n",
      "Working on 2023-3-31\n",
      "Working on 2023-4-1\n",
      "Working on 2023-4-2\n",
      "Working on 2023-4-3\n",
      "Working on 2023-4-4\n",
      "Working on 2023-4-5\n",
      "Working on 2023-4-6\n",
      "Working on 2023-4-7\n",
      "Working on 2023-4-8\n",
      "Working on 2023-4-9\n",
      "Working on 2023-4-10\n",
      "Working on 2023-4-11\n",
      "Working on 2023-4-12\n",
      "Working on 2023-4-13\n",
      "Working on 2023-4-14\n",
      "Working on 2023-4-15\n",
      "Working on 2023-4-16\n",
      "Working on 2023-4-17\n",
      "Working on 2023-4-18\n",
      "Working on 2023-4-19\n",
      "Working on 2023-4-20\n",
      "Working on 2023-4-21\n",
      "Working on 2023-4-22\n",
      "Working on 2023-4-23\n"
     ]
    }
   ],
   "source": [
    "await scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634088af-8a6b-40ce-86a2-be4ff6c7b358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341c406-b0bc-4e78-a468-a6748dff5536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
